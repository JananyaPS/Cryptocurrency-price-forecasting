# -*- coding: utf-8 -*-
"""Copy of FB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BJdyHs4WDcJadCLtVYtBLcmzoaczol1v
"""

import yfinance as yf

ada_data=yf.download(tickers='ADA-USD',start='2017-01-01',end='2024-12-31')
ada_data.columns = ada_data.columns.droplevel(1)
ada_data= ada_data.reset_index()
ada_data

bnb_data =yf.download(tickers='BNB-USD',start='2017-01-01',end='2024-12-31')
bnb_data.columns = bnb_data.columns.droplevel(1)
bnb_data= bnb_data.reset_index()
bnb_data

eth_data =yf.download(tickers='ETH-USD',start='2017-01-01',end='2024-12-31')
eth_data.columns = eth_data.columns.droplevel(1)
eth_data= eth_data.reset_index()
eth_data

import pandas as pd

ada_data = ada_data[['Date','Close']]
bnb_data = bnb_data[['Date','Close']]
eth_data = eth_data[['Date','Close']]

ada_data.rename(columns={'Date': 'timestamp', 'Close': 'adjclose'}, inplace=True)
bnb_data.rename(columns={'Date': 'timestamp', 'Close': 'adjclose'}, inplace=True)
eth_data.rename(columns={'Date': 'timestamp', 'Close': 'adjclose'}, inplace=True)

ada_data = ada_data.sort_values(by='timestamp').reset_index(drop=True)
bnb_data = bnb_data.sort_values(by='timestamp').reset_index(drop=True)
eth_data = eth_data.sort_values(by='timestamp').reset_index(drop=True)

ada_data.head()

bnb_data.head()

eth_data.head()

ada_describe = ada_data.describe()
bnb_describe = bnb_data.describe()
eth_describe = eth_data.describe()

from sklearn.model_selection import train_test_split

ada_train, ada_test = train_test_split(ada_data, test_size=0.26, random_state=42)

# BNB: 70% training, 30% test
bnb_train, bnb_test = train_test_split(bnb_data, test_size=0.30, random_state=42)

# ETH: 74% training, 26% test
eth_train, eth_test = train_test_split(eth_data, test_size=0.26, random_state=42)

import matplotlib.pyplot as plt # Plot the data to visualize the trends
plt.figure(figsize=(12, 8))

# ADA Plot
plt.subplot(3, 1, 1)
plt.plot(ada_data['timestamp'], ada_data['adjclose'], label='ADA', color='blue')
plt.title('ADA: Target Variable Over Time')
plt.xlabel('Date')
plt.ylabel('Target Variable')
plt.grid()
plt.legend()

# BNB Plot
plt.subplot(3, 1, 2)
plt.plot(bnb_data['timestamp'], bnb_data['adjclose'], label='BNB', color='green')
plt.title('BNB: Target Variable Over Time')
plt.xlabel('Date')
plt.ylabel('Target Variable')
plt.grid()
plt.legend()

# ETH Plot
plt.subplot(3, 1, 3)
plt.plot(eth_data['timestamp'], eth_data['adjclose'], label='ETH', color='red')
plt.title('ETH: Target Variable Over Time')
plt.xlabel('Date')
plt.ylabel('Target Variable')
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()

!pip install pmdarima

import numpy as np
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Function to test stationarity using ADF Test
def test_stationarity(timeseries, coin_name):
    result = adfuller(timeseries)
    print(f"ADF Test for {coin_name}:\n")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    for key, value in result[4].items():
        print(f"Critical Value ({key}): {value:.4f}")
    print("Stationary" if result[1] <= 0.05 else "Non-stationary", "\n")

def fit_arima(data, order, coin_name):
    train_data = data['adjclose']
    model = ARIMA(train_data, order=order)
    arima_model = model.fit()

    print(f"ARIMA Model Summary for {coin_name}:\n")
    print(arima_model.summary())

    # Extract residuals
    residuals = arima_model.resid

    # Residual Analysis Plot
    #plt.figure(figsize=(10, 6))
    #plt.plot(residuals)
    #plt.title(f"Residual Analysis for {coin_name}")
    #plt.show()

    print(f"Residual Mean: {np.mean(residuals):.4f}")
    print(f"Residual Variance: {np.var(residuals):.4f}")

    return arima_model, residuals  # Now returning residuals

data_list = [(ada_train, (1, 1, 1), 'ADA'),
             (bnb_train, (5, 2, 5), 'BNB'),
             (eth_train, (2, 1, 3), 'ETH')]

for data, order, coin_name in data_list:
    print(f"Processing {coin_name} Data\n")

    # Test stationarity
    test_stationarity(data['adjclose'], coin_name)

    # Fit ARIMA and get residuals
    arima_model, residuals = fit_arima(data, order, coin_name)

    # Continue with residual analysis
    import seaborn as sns
    from statsmodels.graphics.tsaplots import plot_acf

    # Residual Distribution Plot
    plt.figure(figsize=(10, 6))
    sns.histplot(residuals, kde=True, bins=30, color="blue")
    plt.title(f"Residual Distribution for {coin_name}")
    plt.xlabel("Residuals")
    plt.ylabel("Frequency")
    plt.grid()
    plt.show()

    # Residual ACF Plot
    plt.figure(figsize=(10, 6))
    plot_acf(residuals, lags=40)
    plt.title(f"ACF Plot of Residuals for {coin_name}")
    plt.show()

    # Residual Scatter Plot
    plt.figure(figsize=(10, 6))
    plt.scatter(range(len(residuals)), residuals, alpha=0.5, color="green")
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title(f"Residual Scatter Plot for {coin_name}")
    plt.xlabel("Index")
    plt.ylabel("Residual")
    plt.grid()
    plt.show()

arima_forecast = arima_model.predict(n_periods=30)  # Adjust period as needed
arima_forecast = np.array(arima_forecast)
arima_forecast

import pandas as pd

# Create a DataFrame with predicted values
df_predictions = pd.DataFrame({
    "Date": pd.date_range(start="2025-03-24", periods=len(arima_forecast)),  # Adjust the start date dynamically if needed
    "ARIMA Forecast": arima_forecast
})

# Display the DataFrame
print(df_predictions)

"""FB CANDLESTICK AND MOVING AVERAGES:"""

import pandas as pd
from prophet import Prophet
import plotly.graph_objects as go
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Prepare data for Prophet
def prepare_data_for_prophet(data, coin_name):
    df = data.rename(columns={"timestamp": "ds", "adjclose": "y"})
    print(f"{coin_name} Data Prepared for Prophet:\n")
    print(df.head())
    return df

# Train Prophet model and generate candlestick plot with EMAs
def train_and_forecast_with_plotly(df, coin_name, periods=30):
    # Train the model
    model = Prophet()
    model.fit(df)

    # Create future dataframe
    future = model.make_future_dataframe(periods=periods)
    forecast = model.predict(future)

    # Create candlestick data
    candlestick_data = pd.DataFrame({
        'Date': forecast['ds'],
        'Open': forecast['yhat_lower'],  # Using yhat_lower as Open
        'High': forecast['yhat_upper'],  # Using yhat_upper as High
        'Low': forecast['yhat_lower'],  # Using yhat_lower as Low
        'Close': forecast['yhat']       # Using yhat as Close
    })
    candlestick_data.set_index('Date', inplace=True)

    # Compute EMAs for visualization
    candlestick_data['EMA20'] = candlestick_data['Close'].ewm(span=20, adjust=False).mean()
    candlestick_data['EMA50'] = candlestick_data['Close'].ewm(span=50, adjust=False).mean()

    # Plot candlestick chart with EMAs using Plotly
    fig = go.Figure(data=[
        go.Candlestick(
            x=candlestick_data.index,
            open=candlestick_data['Open'],
            high=candlestick_data['High'],
            low=candlestick_data['Low'],
            close=candlestick_data['Close'],
            name='Candlestick'
        ),
        go.Scatter(
            x=candlestick_data.index,
            y=candlestick_data['EMA20'],
            line=dict(color='red', width=2),
            name='EMA20'
        ),
        go.Scatter(
            x=candlestick_data.index,
            y=candlestick_data['EMA50'],
            line=dict(color='blue', width=2),
            name='EMA50'
        )
    ])

    # Add titles and display the plot
    fig.update_layout(
        title=f"{coin_name}: Forecast with Candlestick Chart and EMAs",
        xaxis_title="Date",
        yaxis_title="Price",
        template="plotly_dark"
    )
    fig.show()

    return forecast

# Evaluate the model
def evaluate_model(df, forecast, coin_name):
    test_actual = df['y']
    test_predicted = forecast.loc[:len(test_actual) - 1, 'yhat']

    mae = mean_absolute_error(test_actual, test_predicted)
    mse = mean_squared_error(test_actual, test_predicted)
    rmse = np.sqrt(mse)

    print(f"{coin_name} Model Evaluation:\n")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}\n")

# Main Workflow
coins = [(ada_train, 'ADA'), (bnb_train, 'BNB'), (eth_train, 'ETH')]

for data, coin_name in coins:
    print(f"Processing {coin_name} Data\n")

    # Step 1: Prepare Data
    prophet_data = prepare_data_for_prophet(data, coin_name)

    # Step 2: Train and Forecast with Candlestick and EMAs
    forecast = train_and_forecast_with_plotly(prophet_data, coin_name)

    # Step 3: Model Evaluation
    evaluate_model(prophet_data, forecast, coin_name)

prophet_forecast = np.array(forecast['yhat'])
prophet_forecast

import pandas as pd

# Create a DataFrame with predicted values
df_prophet_forecast = pd.DataFrame({
    "Date": forecast['ds'],  # Prophet stores dates in 'ds' column
    "Prophet Forecast": forecast['yhat']
})

# Display the DataFrame
print(df_prophet_forecast)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(forecast['ds'], forecast['yhat'], marker='o', linestyle='-', color='g', label="Prophet Forecast")
plt.xlabel("Date")
plt.ylabel("Predicted Value")
plt.title("Prophet Forecasted Bitcoin Prices")
plt.legend()
plt.grid()
plt.show()

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dense

from sklearn.preprocessing import MinMaxScaler
import numpy as np

def preprocess_data(data, column='adjclose'):
    values = data[column].values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(values)
    return scaled_data, scaler

# Function to create sequences
def create_sequences(data, time_steps=60):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i+time_steps])
        y.append(data[i+time_steps])
    return np.array(X), np.array(y)

# Preprocess data for each dataset
time_steps = 60
ada_scaled, ada_scaler = preprocess_data(ada_data)
bnb_scaled, bnb_scaler = preprocess_data(bnb_data)
eth_scaled, eth_scaler = preprocess_data(eth_data)

X_ada, y_ada = create_sequences(ada_scaled, time_steps)
X_bnb, y_bnb = create_sequences(bnb_scaled, time_steps)
X_eth, y_eth = create_sequences(eth_scaled, time_steps)

def build_bidirectional_lstm_model(input_shape):
    model = Sequential()
    model.add(Bidirectional(LSTM(50, return_sequences=True), input_shape=input_shape))
    model.add(Bidirectional(LSTM(50, return_sequences=False)))
    model.add(Dense(25))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

datasets = {'ADA': (X_ada, y_ada, ada_scaler),
            'BNB': (X_bnb, y_bnb, bnb_scaler),
            'ETH': (X_eth, y_eth, eth_scaler)}

results = {}

for name, (X, y, scaler) in datasets.items():
    print(f"Training Bidirectional LSTM model for {name}...")
    model = build_bidirectional_lstm_model((X.shape[1], X.shape[2]))
    model.fit(X, y, batch_size=32, epochs=20, verbose=1)
    results[name] = {
        'model': model,
        'X': X,
        'y': y,
        'scaler': scaler
    }

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_with_candlesticks(model, X, y, scaler, dataset_name, original_dates):
    bilstm_pred = model.predict(X)
    bilstm_pred = scaler.inverse_transform(bilstm_pred)
    bilstm_pred = bilstm_pred.flatten()
    bilstm_pred
    y_actual = scaler.inverse_transform(y.reshape(-1, 1)).flatten()


    mse = mean_squared_error(y_actual, bilstm_pred)
    mae = mean_absolute_error(y_actual, bilstm_pred)
    r2 = r2_score(y_actual, bilstm_pred)

    print(f"\nEvaluation for {dataset_name}:")
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Mean Absolute Error (MAE): {mae}")
    print(f"R-squared (R2): {r2}")

    # Ensure original_dates aligns with y_actual
    date_range = original_dates[-len(y_actual):]  # Keep only the relevant dates

    # Prepare candlestick chart data
    candlestick_data = pd.DataFrame({
        'Date': date_range,
        'Open': y_actual.flatten(),
        'High': np.maximum(y_actual.flatten(), bilstm_pred.flatten()),
        'Low': np.minimum(y_actual.flatten(), bilstm_pred.flatten()),
        'Close': bilstm_pred.flatten()
    })

    # Plot candlestick chart with Plotly
    fig = go.Figure(data=[
        go.Candlestick(
            x=candlestick_data['Date'],
            open=candlestick_data['Open'],
            high=candlestick_data['High'],
            low=candlestick_data['Low'],
            close=candlestick_data['Close'],
            name='Candlestick'
        )
    ])
    fig.update_layout(
        title=f"{dataset_name}: Actual vs Predicted (Candlestick Chart)",
        xaxis_title="Time",
        yaxis_title="Close Price",
        xaxis=dict(type='date'),
        template="plotly_dark"
    )
    fig.show()

    # Line plot for Actual vs Predicted
    plt.figure(figsize=(10, 6))
    plt.plot(date_range, y_actual, label='Actual', color='blue')
    plt.plot(date_range, bilstm_pred, label='Predicted', color='orange', alpha=0.7)
    plt.title(f"Actual vs Predicted Close Prices for {dataset_name}")
    plt.xlabel("Time")
    plt.ylabel("Close Price")
    plt.xticks(rotation=45)  # Rotate dates for better visibility
    plt.legend()
    plt.grid()
    plt.show()

bilstm_predictions = {}

for name, data in results.items():
    model = data['model']  # Trained Bi-LSTM model
    X_test = data['X']      # Test data for prediction
    y_test = data['y']      # Actual values
    scaler = data['scaler']  # Scaler for inverse transformation

    # Generate predictions
    bilstm_pred = model.predict(X_test)
    bilstm_pred = scaler.inverse_transform(bilstm_pred)  # Convert to original scale
    bilstm_pred = bilstm_pred.flatten()  # Convert to 1D array

    # Store results
    bilstm_predictions[name] = bilstm_pred

bilstm_pred

for name, result in results.items():
    if name == "ADA":
        original_dates = ada_data["timestamp"]
    elif name == "BNB":
        original_dates = bnb_data["timestamp"]
    elif name == "ETH":
        original_dates = eth_data["timestamp"]

    original_dates = pd.to_datetime(original_dates).values  # Convert to datetime
    evaluate_with_candlesticks(result['model'], result['X'], result['y'], result['scaler'], name, original_dates)

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import numpy as np

# Preprocess Data
def preprocess_data(data, column='adjclose'):
    values = data[column].values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(values)
    return scaled_data, scaler

# Create sequences
def create_sequences(data, time_steps=60):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i+time_steps])
        y.append(data[i+time_steps])
    return np.array(X), np.array(y)

# Build Stacked LSTM Model
def build_stacked_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))
    model.add(LSTM(50, return_sequences=True))  # Second stacked LSTM layer
    model.add(LSTM(50, return_sequences=False))  # Final LSTM layer
    model.add(Dense(25))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Prepare datasets
time_steps = 60
ada_scaled, ada_scaler = preprocess_data(ada_data)
bnb_scaled, bnb_scaler = preprocess_data(bnb_data)
eth_scaled, eth_scaler = preprocess_data(eth_data)

X_ada, y_ada = create_sequences(ada_scaled, time_steps)
X_bnb, y_bnb = create_sequences(bnb_scaled, time_steps)
X_eth, y_eth = create_sequences(eth_scaled, time_steps)

datasets = {'ADA': (X_ada, y_ada, ada_scaler),
            'BNB': (X_bnb, y_bnb, bnb_scaler),
            'ETH': (X_eth, y_eth, eth_scaler)}

results = {}

# Train the models
for name, (X, y, scaler) in datasets.items():
    print(f"Training Stacked LSTM model for {name}...")
    model = build_stacked_lstm_model((X.shape[1], X.shape[2]))
    model.fit(X, y, batch_size=32, epochs=20, verbose=1)
    results[name] = {
        'model': model,
        'X': X,
        'y_lstm': y,
        'scaler': scaler
    }

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_with_candlesticks(model, X, y, scaler, dataset_name, timestamps):
    y_pred = model.predict(X)
    y_pred = scaler.inverse_transform(y_pred).flatten()
    y_actual = scaler.inverse_transform(y.reshape(-1, 1)).flatten()

    mse = mean_squared_error(y_actual, y_pred)
    mae = mean_absolute_error(y_actual, y_pred)
    r2 = r2_score(y_actual, y_pred)

    print(f"\nEvaluation for {dataset_name}:")
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Mean Absolute Error (MAE): {mae}")
    print(f"R-squared (R2): {r2}")

    # Convert timestamps to datetime if not already
    timestamps = pd.to_datetime(timestamps)

    # Create candlestick data
    candlestick_data = pd.DataFrame({
        'Date': timestamps[-len(y_actual):],  # Match timestamps to actual data length
        'Open': y_actual.flatten(),
        'High': np.maximum(y_actual.flatten(), y_pred.flatten()),
        'Low': np.minimum(y_actual.flatten(), y_pred.flatten()),
        'Close': y_pred.flatten()
    })

    # Plot candlestick chart with Plotly
    fig = go.Figure(data=[
        go.Candlestick(
            x=candlestick_data['Date'],
            open=candlestick_data['Open'],
            high=candlestick_data['High'],
            low=candlestick_data['Low'],
            close=candlestick_data['Close'],
            name='Candlestick'
        )
    ])
    fig.update_layout(
        title=f"{dataset_name}: Actual vs Predicted (Candlestick Chart)",
        xaxis_title="Year",
        yaxis_title="Close Price",
        xaxis=dict(type="date"),  # Ensure x-axis shows actual dates
        template="plotly_dark"
    )
    fig.show()

    # Line plot for Actual vs Predicted
    plt.figure(figsize=(12, 6))
    plt.plot(timestamps[-len(y_actual):], y_actual, label='Actual', color='blue')
    plt.plot(timestamps[-len(y_actual):], y_pred, label='Predicted', color='orange', alpha=0.7)
    plt.title(f"Actual vs Predicted Close Prices for {dataset_name}")
    plt.xlabel("Year")
    plt.ylabel("Close Price")
    plt.legend()
    plt.grid()
    plt.xticks(rotation=45)  # Rotate for better visibility
    plt.show()

# Ensure the dataset's timestamp is converted to datetime
for name, result in results.items():
    if name == "ADA":
        original_dates = ada_data["timestamp"]
    elif name == "BNB":
        original_dates = bnb_data["timestamp"]
    elif name == "ETH":
        original_dates = eth_data["timestamp"]

    original_dates = pd.to_datetime(original_dates).values  # Convert to datetime
    evaluate_with_candlesticks(result['model'], result['X'], result['y_lstm'], result['scaler'], name, original_dates)

slstm_predictions = {}

for name, data in results.items():
    slstm_model = data['model']  # Trained Bi-LSTM model
    X_test = data['X']      # Test data for prediction
    y_test = data['y_lstm']      # Actual values
    scaler = data['scaler']  # Scaler for inverse transformation

    # Generate predictions
    y_pred = model.predict(X_test)
    y_pred = scaler.inverse_transform(y_pred)  # Convert to original scale
    y_pred = y_pred.flatten()  # Convert to 1D array

    # Store results
    slstm_predictions[name] = y_pred

y_pred

# Ensure all predictions have the same length
min_length = min(len(arima_forecast), len(prophet_forecast), len(bilstm_pred), len(y_pred))

# Truncate all arrays to match the shortest length
arima_pred = arima_forecast[:min_length]
prophet_pred = prophet_forecast[:min_length]
bilstm_pred = bilstm_pred[:min_length]
y_pred = y_pred[:min_length]

# Compute weighted average
final_prediction = (arima_pred + prophet_pred + bilstm_pred + y_pred) / 4

# Display the result
print("Final Weighted Average Prediction:")
print(final_prediction)

import pandas as pd

# Ensure all predictions have the same length
min_length = min(len(arima_forecast), len(prophet_forecast), len(bilstm_pred), len(y_pred))

# Truncate all arrays to match the shortest length
arima_pred = arima_forecast[:min_length]
prophet_pred = prophet_forecast[:min_length]
bilstm_pred = bilstm_pred[:min_length]
y_pred = y_pred[:min_length]

# Compute weighted average
final_prediction = (arima_pred + prophet_pred + bilstm_pred + y_pred) / 4

# Create DataFrame
predictions_df = pd.DataFrame({
    "ARIMA Prediction": arima_pred,
    "Prophet Prediction": prophet_pred,
    "Bi-LSTM Prediction": bilstm_pred,
    "LSTM Prediction": y_pred,
    "Final Weighted Prediction": final_prediction
})

# Display DataFrame
print("\nFinal Predictions DataFrame:")
print(predictions_df)

import pandas as pd
from datetime import datetime, timedelta

# Ensure all predictions have the same length
min_length = min(len(arima_forecast), len(prophet_forecast), len(bilstm_pred), len(y_pred))

# Truncate all arrays to match the shortest length
arima_pred = arima_forecast[:min_length]
prophet_pred = prophet_forecast[:min_length]
bilstm_pred = bilstm_pred[:min_length]
y_pred = y_pred[:min_length]

# Compute weighted average
final_prediction = (arima_pred + prophet_pred + bilstm_pred + y_pred) / 4

# Create a date range (adjust the start date as needed)
start_date = datetime(2024, 1, 1)
dates = [start_date + timedelta(days=i) for i in range(min_length)]

# Define crypto being predicted (change as needed)
crypto_name = "ETH"

# Create DataFrame with additional columns
predictions_df = pd.DataFrame({
    "Date": dates,
    "Crypto": crypto_name,
    "ARIMA Prediction": arima_pred,
    "Prophet Prediction": prophet_pred,
    "Bi-LSTM Prediction": bilstm_pred,
    "LSTM Prediction": y_pred,
    "Final Weighted Prediction": final_prediction
})

# Save to CSV
predictions_df.to_csv("predictions.csv", index=False)

# Optionally print confirmation
print("\nPredictions saved to 'predictions.csv'")